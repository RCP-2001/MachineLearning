{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SP23:CS 477/577: Python for Machine Learning\n",
    "\n",
    "### HW2: Use logistic regression for classification\n",
    "\n",
    "\n",
    "Student Name: Last, First"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preliminary\n",
    "\n",
    "#### 1.1 Problem formulation\n",
    "1. Training set. Data sampels: X_train = {$x_1, x_2, ..., x_i, ..., x_m$}. Target (class labels):Y_train = {$y_1, y_2, ..., y_i, ..., y_m$}\n",
    "    \n",
    "2. Predicted class probability in logistic regression:\n",
    "\n",
    "\\begin{equation*} \n",
    "    \\hat{y_i} = 1/(1+exp-(w_0 + w_1*x_{i,1} + ,..., +w_n*x_{i,n}))\n",
    "\\end{equation*}    \n",
    "    where $w_0, w_1,...,w_n$ are the linear model parameters, and $ x_{i,1}..., x_{i,n}$ are the features of the ith data feature vector $x_i$\n",
    "\n",
    "3. Cost/loss function of the logistic regression: the criterion to quantitatively evaluate how good the current model is (the less the better).\n",
    "\\begin{equation*}\n",
    "    J(w) = -C [\\sum_{i=1}^m (y_i \\times log(\\hat{y_i}) + (1-y_i) \\times log(1-\\hat{y_i}))] \n",
    "\\end{equation*}\n",
    "    - In the above equation, $\\hat{y_i}$  and  $y_i$ are the predicted class lable and the true class label of data sample $x_i$, respectively.\n",
    "    - P is the penalty/regularizer, and C controls the contribution of P. There are three typical options of P: L1 norm ($ \\|w\\|_1$), L2 norm ($\\frac{1}{2}w^T w$) and elstic-Net($\\frac{1 - \\rho}{2}w^T w + \\rho \\|w\\|_1$)\n",
    "    \n",
    "4. Model training is to use an 'optimization algorithm' to find the best $w$ that can minimize the loss function $J(w)$. Refer to the gradient descent algorithm in the Other Learning materials for more information.\n",
    "\n",
    "#### 1.2. The sklearn.linear_model.LogisticRegression class\n",
    "\n",
    "1. class introduction: Key attributes (e.g., C, and penalty)and methods (fit, predict, predict_proba, and score)\n",
    "    - https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#\n",
    "    \n",
    "2. source code\n",
    "    - https://github.com/scikit-learn/scikit-learn/blob/b194674c4/sklearn/linear_model/_logistic.py#L1191"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Breast Cancer Detection\n",
    "\n",
    "Develop the logistic regression approach to classify breast cases into malignat(cancer) and benign.\n",
    "\n",
    "Dataset\n",
    "\n",
    "    :Number of data samples: 569\n",
    "    :Number of features: 30 numeric. The first 10 features were directly calculated using mean feautues of all nuclei in an image\n",
    "    :Class labels (We changed the original class labels by using 1 - y.)\n",
    "        : Malignant: 1 (malignant or cancer)\n",
    "        : Benign: 0  (benign or non-cancer)  \n",
    "    https://scikit-learn.org/stable/datasets/index.html#breast-cancer-dataset\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2.1\n",
      "(569, 30) (569,)\n",
      "training samples (455, 30) test samples (114, 30)\n"
     ]
    }
   ],
   "source": [
    "import sklearn.datasets as ds\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import sklearn\n",
    "print(sklearn.__version__)\n",
    "\n",
    "# 1) data preparation\n",
    "X,y = ds.load_breast_cancer(return_X_y=True)\n",
    "y = 1-y\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "rs = 0\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = rs)\n",
    "print('training samples', X_train.shape, 'test samples', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Train and explore a logistic regression model. 25 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.70193886 -0.18897771  0.18942047 -0.02287614  0.14411881  0.19150739\n",
      "   0.37130293  0.20725414  0.26954863  0.03052996  0.04613319 -1.02791655\n",
      "   0.03333033  0.10636898  0.01239678 -0.04904355  0.02211939  0.02255294\n",
      "   0.03059153 -0.01346839 -0.26372385  0.41354888  0.19954951  0.0110381\n",
      "   0.25778885  0.68276195  1.22299877  0.44846203  0.62294273  0.0968387 ]]\n",
      "[[9.99605374e-01 3.94626224e-04]\n",
      " [9.66264322e-01 3.37356777e-02]\n",
      " [9.44933289e-01 5.50667111e-02]\n",
      " [9.99795932e-01 2.04068351e-04]\n",
      " [8.90069867e-01 1.09930133e-01]]\n"
     ]
    }
   ],
   "source": [
    "#1) train a logistic regression model. 5 points\n",
    "model = LogisticRegression(random_state=rs, max_iter=3000).fit(X= X_train, y=y_train)\n",
    "\n",
    "\n",
    "#2) print out the optimal model(linear) parameters after the training. 5 points\n",
    "            #w0\n",
    "            #w1...n\n",
    "params = model.coef_\n",
    "print(params)\n",
    "\n",
    "#3) calculate and print out the predicted class probabilities of \n",
    "#the first 5 training samples by implementing the equation in 1.1 (y_i^hat). 10 points\n",
    "\n",
    "\n",
    "\n",
    "#4) calculate and print out the predicted class probabilities of \n",
    "#the first 5 training samples using the predict_proba() function. 5 points\n",
    "probablity = model.predict_proba(X_train[:5])\n",
    "print(probablity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Evaluation 1.0. 25 points\n",
    "\n",
    "1) Accuracy. 5 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9626373626373627\n",
      "0.9473684210526315\n"
     ]
    }
   ],
   "source": [
    "# calculate and print out the accuracy of the trained model on the training set\n",
    "# using the score function\n",
    "print(model.score(X_train, y_train))\n",
    "\n",
    "\n",
    "# calculate and print out the accuracy of the trained model on the test set\n",
    "# using the score function\n",
    "print(model.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Confusion matrix. 5 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[62  5]\n",
      " [ 1 46]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "#print out the confusion matrix (cfm) for the trained model on the test set\n",
    "pred = model.predict(X_test)\n",
    "cfm = confusion_matrix(y_test , pred)\n",
    "print(cfm)\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) What is your observation from the cfm? 5 points\n",
    "\n",
    "Response (add your response here): \n",
    "   1) \n",
    "   \n",
    "   2)\n",
    "   \n",
    "   ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Recall and Precision. 10 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print out the recall ratio of the trained model for \n",
    "#detecting breast cancer using the above cfm\n",
    "\n",
    "\n",
    "# print out the precision of the trained model for \n",
    "#detecting breast cancer using the above cfm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  2.4 Evaluation 2.0. 30 points\n",
    "\n",
    "K-fold cross validation. (See Lecture Notes 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Why do we need K-fold cross validation in this application? 5 points\n",
    "\n",
    "    Reponse(add your response here):\n",
    "    \n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Use the 5-fold cross validation to re-evaluate a logistic regression model. 15 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rcp_2001/anaconda3/envs/MachineLearning2/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/rcp_2001/anaconda3/envs/MachineLearning2/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/rcp_2001/anaconda3/envs/MachineLearning2/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/rcp_2001/anaconda3/envs/MachineLearning2/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/rcp_2001/anaconda3/envs/MachineLearning2/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.05786514, 0.02947497, 0.02283478, 0.02210546, 0.02156639]),\n",
       " 'score_time': array([0.00324845, 0.00143147, 0.00139356, 0.00147128, 0.00140595]),\n",
       " 'test_accuracy': array([0.9122807 , 0.92982456, 0.90350877, 0.92105263, 0.89380531]),\n",
       " 'test_recall': array([0.79069767, 0.86046512, 0.78571429, 0.80952381, 0.78571429])}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cross_validate in sklearn should be used in this task\n",
    "from sklearn.model_selection import cross_validate as cv\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "myLR_1 = LogisticRegression(C = 5, penalty = 'l1', solver = 'saga', random_state = rs)\n",
    "\n",
    "#call the cross_validate function and set scoring = ('accuracy', 'recall')\n",
    "k = 5\n",
    "cv(myLR_1, X, y, cv=k, scoring = ('accuracy', 'recall'))\n",
    "\n",
    "#print out the test accuracy for each fold and print out the mean accuracy \n",
    "\n",
    "\n",
    "#print out the test reacll ratios for each fold and print out the mean recall ratio \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) What are the differences between Evaluation 1.0 and Evaluation 2.0? Which one is better for this application? and why? 10 points\n",
    "\n",
    "Response (Add your responses here):\n",
    "\n",
    "(1)\n",
    "\n",
    "(2)\n",
    "\n",
    "...\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 2.5 How can we improve the recall ratio?  20 points.\n",
    "\n",
    "Achieving high recall ratio(RR) is very important is cancer detection. Because RR indicates a model's ability to identify cancers; and low recall ratio may cost the lives of patients. Can you help improve the logistic regression approach to increse the recall ratio?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mxian/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=5, penalty='l1', random_state=0, solver='saga')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train a logistic regression model.\n",
    "myLR_2 = LogisticRegression(C = 5, penalty = 'l1', solver = 'saga', random_state = rs)\n",
    "myLR_2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Print out the predicted probabilities of all misclassified samples (test set) and print out their class labels. 5 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Define a new prediction function that use threshod $th=0.37$ to determine the final class label (0 0r 1). 10 points\n",
    "\n",
    "Tips: if the predict_proba function returns a probability that is less than $th$ the new prediction function returns 0, otherwise returns 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pred_b' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-4c887b0aaab6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpred_b\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmyPredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyLR_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.35\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mcm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-4c887b0aaab6>\u001b[0m in \u001b[0;36mmyPredict\u001b[0;34m(lr, X, th)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mpass\u001b[0m \u001b[0;31m# delete pass after you complete the code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpred_b\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmyPredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyLR_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.35\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pred_b' is not defined"
     ]
    }
   ],
   "source": [
    "def myPredict(lr, X, th = 0.35):\n",
    "    '''  \n",
    "        input:\n",
    "            lr: a trained logistic regression model\n",
    "            X: input feature vectors\n",
    "            th: threshold. \n",
    "\n",
    "        return predicted binary labels for all inuput samples\n",
    "    '''    \n",
    "    #add your code here\n",
    "    pass # delete pass after you complete the code\n",
    "\n",
    "    #\n",
    "    return pred_b\n",
    "\n",
    "y_pred = myPredict(myLR_2, X_test, th = 0.35)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "#print out the accuracy and the recall ratio on the test set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Other possible ideas could be applied to improve the recall ratio of the logistic regression. 5 points\n",
    "\n",
    "Response:\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MachineLearning2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "17816603dc05db234063064a581facafe56897b7a0103b91884cec605ab64bc2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
